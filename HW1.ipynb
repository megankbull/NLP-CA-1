{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Function Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python version 3.10.6\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True) \n",
    "nltk.download('punkt', quiet=True) \n",
    "\n",
    "from textacy.preprocessing import remove, normalize, replace\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "\n",
    "import warnings \n",
    "import contractions\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS \n",
    "\n",
    "F_PATH = 'amazon_reviews_us_Jewelry_v1_00.tsv'\n",
    "\n",
    "STAR_H = 'star_rating'\n",
    "REVIEW_H = 'review_body'\n",
    "\n",
    "COLS=[STAR_H, REVIEW_H]\n",
    "\n",
    "VAL_STARS = {'1', '2', '3', '4', '5'}\n",
    "\n",
    "WNL = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(f_path=F_PATH):\n",
    "   df = pd.read_csv(f_path, sep='\\t', usecols=COLS, low_memory=False)\n",
    "   df.dropna(inplace=True)\n",
    "   return df\n",
    "\n",
    "def get_sample(df, s_size=20000):\n",
    "\n",
    "   grouped = df.groupby(STAR_H)\n",
    "   rat_dfs = [grouped.get_group(rating).sample(n=s_size) for rating in VAL_STARS]\n",
    "   return pd.concat(rat_dfs) \n",
    "\n",
    "def gen_clean(text):\n",
    "   \"\"\"\n",
    "   gen text cleanup \n",
    "   incl removal: extended ws, html tags, urls\n",
    "   \"\"\"\n",
    "   text = BeautifulSoup(text, \"html.parser\").text #rm html tags \n",
    "   text = replace.urls(text, '')\n",
    "   text = contractions.fix(text)\n",
    "   text = remove.punctuation(text)\n",
    "   text = normalize.whitespace(text)\n",
    "   \n",
    "   return text.lower()\n",
    "   \n",
    "def rm_stops(text): \n",
    "   \"\"\"\n",
    "   remove stop words from text \n",
    "   \"\"\"\n",
    "   stops = set(stopwords.words(\"english\"))\n",
    "   sans_stops = [tok for tok in word_tokenize(text) if tok not in stops]\n",
    "   return \" \".join(sans_stops).strip()\n",
    "\n",
    "def lemmatize(text): \n",
    "\n",
    "   lemmas = [WNL.lemmatize(w) for w in word_tokenize(text)]\n",
    "   return \" \".join(lemmas).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "1. Read from file.\n",
    "2. Drop NaN vals.\n",
    "3. Visually verify unique `star_rating` values are valid rating classes to determine if more rows need to be dropped. \n",
    "4. Randomly select 20,000 samples frome each valid rating class \n",
    "   - This is done by grouping the original DataFrame by `VAL_STARS`, list of sampled DataFrames from each rating class, and finally, concatenating them together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5' '1' '4' '3' '2']\n"
     ]
    }
   ],
   "source": [
    "df = read_data()\n",
    "print(df[STAR_H].unique())\n",
    "\n",
    "sampled = get_sample(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "1. Perform general text cleaning including: \n",
    "   - Remove html tags via `BeautifulSoup`'s `html.parser`\n",
    "   - Remove URLS via `textaCy`'s `.replace.urls` functionality.\n",
    "   - Resolve contractions with `contractions` library which handles a host of contractions, including slang such as 'y'all'.\n",
    "   - Replace punctuation with `textaCy` which replaces all instances of punctuation. \n",
    "   - Normalize whitespace via `textaCy` which replaces all contiguous zero-width spaces as well as strip leading and trailing whitespace. \n",
    "   - Convert all text to lowercase. \n",
    "2. Print Average character length of reviews pre and post-clean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189.68736, 183.72778\n"
     ]
    }
   ],
   "source": [
    "raw_len_avg = sampled[REVIEW_H].str.len().mean()\n",
    "\n",
    "sampled[REVIEW_H] = sampled[REVIEW_H].apply(gen_clean)\n",
    "\n",
    "cl_len_avg = sampled[REVIEW_H].str.len().mean()\n",
    "\n",
    "print(f'{raw_len_avg}, {cl_len_avg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "1. Remove english stop words using `NLTK`'s set of stop words and performing list comprhension to for a list of remaining words. <br>\n",
    "   Returns a stripped string of the concatenated words to account for stops appearing at the beginning or end of a review. \n",
    "2. Perform lemmatization using `WordNetLemmatizer` on tokens produced by `nltk.tokenize.word_tokenize`\n",
    "3. Print average character length before and after pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183.72778, 108.92808\n"
     ]
    }
   ],
   "source": [
    "sampled[REVIEW_H] = sampled[REVIEW_H].apply(rm_stops)\n",
    "sampled[REVIEW_H] = sampled[REVIEW_H].apply(lemmatize)\n",
    "\n",
    "preproc_len_avg = sampled[REVIEW_H].str.len().mean()\n",
    "print(f'{cl_len_avg}, {preproc_len_avg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Feature Extraction\n",
    "`use_idf` is included as a parameter due to the findings of the `GridSearchCV` algorithm from `sklearn`. This parameter disables reweighting due to IDF. My original implementation had this parameter set to `True`, however, I noticed an increase in overall average precision across all of the algorithms with the use of `use_idf = False`, necessitating its' inclusion in my performance report due to the competitive nature of grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = TfidfVectorizer(use_idf=False)\n",
    "feat = v.fit_transform(sampled[REVIEW_H])\n",
    "X_train, X_test, train_labels, test_labels = train_test_split(feat, sampled[STAR_H], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "Hyperparameters were tuned using `GridSearchCV` from the `sklearn` package. Since the code for tuning is not relevant to the results it has not been included.\n",
    "I found that on average, the perceptron algorithm performed less than 20 iterations and `GridSearchCV` helped me determine the best boundary for the `max_iter` parameter. `random_state` is included for reproducability of results and `class_weight` being set to `balanced` allows the algorithm to adjust for unbalanced datasets. My inclination to include this last parameter stemmed from the conclusion that the train/test split made after TF-IDF would not be proportionally represetative of the rating classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.48      0.48      4015\n",
      "           2       0.33      0.29      0.31      4018\n",
      "           3       0.31      0.22      0.26      3987\n",
      "           4       0.33      0.49      0.40      3978\n",
      "           5       0.52      0.51      0.52      4002\n",
      "\n",
      "    accuracy                           0.40     20000\n",
      "   macro avg       0.40      0.40      0.39     20000\n",
      "weighted avg       0.40      0.40      0.39     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = Perceptron(random_state=42, class_weight='balanced', max_iter=20, n_iter_no_change=3)\n",
    "p.fit(X_train, train_labels)\n",
    "p_pred = p.predict(X_test)\n",
    "print(classification_report(test_labels, p_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "As with the perceptron algorithm, I tuned SVM using `GridSearchCV`. According to the `sklearn` documentation, `dual` should prefer to be `False` when the number of samples is greater than the number of features, which is true in this case. As mentioned above, `random_state` is included for reproducability and is set with 42, as per the recommendation of the `sklearn` glossary. The grid search tested a number of `max_iter` values but ultimately found this led to the best performance without risk of overfitting. `penalty` is set to `l1` since our features are sparse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.68      0.61      4015\n",
      "           2       0.40      0.33      0.36      4018\n",
      "           3       0.42      0.35      0.38      3987\n",
      "           4       0.46      0.43      0.44      3978\n",
      "           5       0.62      0.75      0.68      4002\n",
      "\n",
      "    accuracy                           0.51     20000\n",
      "   macro avg       0.49      0.51      0.49     20000\n",
      "weighted avg       0.49      0.51      0.50     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(penalty='l1', dual=False, random_state=42, max_iter=300)\n",
    "svm.fit(X_train, train_labels)\n",
    "svm_pred = svm.predict(X_test)\n",
    "print(classification_report(test_labels, svm_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Again, hyperparameters were tuned with the use of `GridSearchCV`. Grid search was especially helpful in this case due to the variety of solvers that can be used in this algorithm. Out of the four mentioned in the documentation to be most suitable for multiclass problems, grid search found `sag` to be optimal. However, it should be noted the difference in performance between solvers was not significant. `max_iter` is set to 400 instead of the default 100 due to warnings of convergence failure on lower max iterations. `class_weight` set to `balanced` also appeared to improve this algorithm and was included with the grid search results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.64      0.61      4015\n",
      "           2       0.42      0.38      0.40      4018\n",
      "           3       0.42      0.40      0.41      3987\n",
      "           4       0.47      0.45      0.46      3978\n",
      "           5       0.65      0.71      0.68      4002\n",
      "\n",
      "    accuracy                           0.52     20000\n",
      "   macro avg       0.51      0.52      0.51     20000\n",
      "weighted avg       0.51      0.52      0.51     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=42, max_iter=400, class_weight='balanced', solver='sag')\n",
    "lr.fit(X_train, train_labels)\n",
    "lr_pred = lr.predict(X_test)\n",
    "print(classification_report(test_labels, lr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "Although not needed due to the lack of variance in parameters as compared to previous algorithms, `GridSearchCV` was also used to tune hyperparameters. Setting `alpha=1` is representative of an added smoothing parameter while `fit_prior=False` disallows the algorithm from learning class prior probabilites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.62      0.61      4015\n",
      "           2       0.42      0.39      0.41      4018\n",
      "           3       0.42      0.43      0.42      3987\n",
      "           4       0.45      0.44      0.45      3978\n",
      "           5       0.65      0.68      0.66      4002\n",
      "\n",
      "    accuracy                           0.51     20000\n",
      "   macro avg       0.51      0.51      0.51     20000\n",
      "weighted avg       0.51      0.51      0.51     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB(alpha=1, fit_prior=False)\n",
    "nb.fit(X_train, train_labels)\n",
    "nb_pred = nb.predict(X_test)\n",
    "print(classification_report(test_labels, nb_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
